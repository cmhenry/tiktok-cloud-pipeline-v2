#!/bin/bash
# Transfer Worker Setup Script
# Run as root on the coordinator VM after cloning repo to /opt/pipeline
#
# Prerequisites:
#   - Redis and PostgreSQL already running (via setup-coordinator.sh)
#   - SSH access to tt-zrh configured in ~ubuntu/.ssh/config
#   - rclone installed
#
# Usage:
#   git clone https://github.com/cmhenry/tiktok-cloud-pipeline-v2 /opt/pipeline
#   cd /opt/pipeline
#   # Edit configuration below
#   sudo ./deploy/setup-transfer-worker.sh

set -e

# =============================================================================
# 1. CONFIGURATION - Update these values before running
# =============================================================================

# Redis (localhost on coordinator)
REDIS_HOST="${REDIS_HOST:-127.0.0.1}"
REDIS_PORT="${REDIS_PORT:-6379}"

# PostgreSQL (localhost on coordinator)
POSTGRES_HOST="${POSTGRES_HOST:-127.0.0.1}"
POSTGRES_PORT="${POSTGRES_PORT:-5432}"
POSTGRES_DB="${POSTGRES_DB:-transcript_db}"
POSTGRES_USER="${POSTGRES_USER:-transcript_user}"
POSTGRES_PASSWORD="${POSTGRES_PASSWORD:-transcript_pass}"

# S3/Object Storage (OpenStack Swift compatible)
S3_ENDPOINT="${S3_ENDPOINT:-}"
S3_ACCESS_KEY="${S3_ACCESS_KEY:-}"
S3_SECRET_KEY="${S3_SECRET_KEY:-}"
S3_BUCKET="${S3_BUCKET:-audio_pipeline}"

# Paths
VOLUME_ROOT="${VOLUME_ROOT:-/mnt/data}"
LOG_DIR="${LOG_DIR:-/home/ubuntu/log/pipeline}"

# =============================================================================
# SCRIPT START - No changes needed below
# =============================================================================

echo "=== Transfer Worker Setup ==="
echo ""

# Check for root
if [[ $EUID -ne 0 ]]; then
   echo "ERROR: This script must be run as root (use sudo)"
   exit 1
fi

# Check we're running from the repo
if [[ ! -f "/opt/pipeline/requirements.txt" ]]; then
    echo "ERROR: Script must be run from /opt/pipeline"
    echo "Clone the repo first: git clone <repo> /opt/pipeline"
    exit 1
fi

# =============================================================================
# 2. UPDATE CODE
# =============================================================================
echo "[1/8] Updating code at /opt/pipeline..."
git -C /opt/pipeline pull || echo "  WARNING: git pull failed (offline or detached HEAD)"

# =============================================================================
# 3. SYSTEM PACKAGES
# =============================================================================
echo "[2/8] Installing system packages..."
apt-get update -qq
apt-get install -y -qq python3-venv python3-pip rclone git

# =============================================================================
# 4. PYTHON VIRTUAL ENVIRONMENT
# =============================================================================
echo "[3/8] Setting up Python environment..."

python3 -m venv /opt/pipeline/venv
source /opt/pipeline/venv/bin/activate

pip install --upgrade pip -q
pip install -r /opt/pipeline/requirements.txt -q

echo "  Python environment ready at /opt/pipeline/venv"

# =============================================================================
# 5. CREATE DIRECTORIES
# =============================================================================
echo "[4/8] Creating directory structure..."

mkdir -p "$VOLUME_ROOT"/{incoming,unpacked,audio,processed}
mkdir -p "$LOG_DIR"
mkdir -p /home/ubuntu/transfer_locks

chown -R ubuntu:ubuntu "$VOLUME_ROOT"
chown -R ubuntu:ubuntu "$LOG_DIR"
chown -R ubuntu:ubuntu /home/ubuntu/transfer_locks

echo "  Directory structure:"
echo "    $VOLUME_ROOT       - Shared volume"
echo "    $LOG_DIR           - Log files"
echo "    ~/transfer_locks   - File transfer locks"

# =============================================================================
# 6. GENERATE .env FILE
# =============================================================================
echo "[5/8] Generating .env file..."

cat > /opt/pipeline/.env <<EOF
# Transfer Worker Configuration (Coordinator VM)
# Generated by setup-transfer-worker.sh on $(date -Iseconds)

# Redis (local)
REDIS_HOST=${REDIS_HOST}
REDIS_PORT=${REDIS_PORT}

# PostgreSQL (local)
POSTGRES_HOST=${POSTGRES_HOST}
POSTGRES_PORT=${POSTGRES_PORT}
POSTGRES_DB=${POSTGRES_DB}
POSTGRES_USER=${POSTGRES_USER}
POSTGRES_PASSWORD=${POSTGRES_PASSWORD}

# S3/Object Storage
S3_ENDPOINT=${S3_ENDPOINT}
S3_ACCESS_KEY=${S3_ACCESS_KEY}
S3_SECRET_KEY=${S3_SECRET_KEY}
S3_BUCKET=${S3_BUCKET}

# Paths
VOLUME_ROOT=${VOLUME_ROOT}
LOG_DIR=${LOG_DIR}
EOF

chmod 600 /opt/pipeline/.env
chown ubuntu:ubuntu /opt/pipeline/.env
echo "  Created /opt/pipeline/.env"

# =============================================================================
# 7. INSTALL SYSTEMD SERVICE
# =============================================================================
echo "[6/8] Installing systemd service..."

cp /opt/pipeline/deploy/systemd/transfer-worker.service /etc/systemd/system/
systemctl daemon-reload
echo "  Installed transfer-worker.service"

# =============================================================================
# 8. INSTALL HEALTH CHECK
# =============================================================================
echo "[7/8] Installing health check script..."

cp /opt/pipeline/deploy/check-health.sh /opt/pipeline/check-health.sh
chmod +x /opt/pipeline/check-health.sh
chown ubuntu:ubuntu /opt/pipeline/check-health.sh
echo "  Installed check-health.sh"

# =============================================================================
# 9. VERIFICATION
# =============================================================================
echo "[8/8] Verifying setup..."
echo ""

ERRORS=0

# Check Python venv
if [[ -f /opt/pipeline/venv/bin/python ]]; then
    echo "  [OK] Python venv exists"
else
    echo "  [FAIL] Python venv missing"
    ERRORS=$((ERRORS+1))
fi

# Check .env
if [[ -f /opt/pipeline/.env ]]; then
    echo "  [OK] .env file exists"
else
    echo "  [FAIL] .env file missing"
    ERRORS=$((ERRORS+1))
fi

# Check systemd service
if systemctl list-unit-files | grep -q transfer-worker.service; then
    echo "  [OK] transfer-worker.service installed"
else
    echo "  [FAIL] transfer-worker.service not found"
    ERRORS=$((ERRORS+1))
fi

# Check rclone
if command -v rclone &> /dev/null; then
    echo "  [OK] rclone installed"
else
    echo "  [FAIL] rclone not installed"
    ERRORS=$((ERRORS+1))
fi

# Check SSH config for tt-zrh
if sudo -u ubuntu ssh -F /home/ubuntu/.ssh/config -o BatchMode=yes -o ConnectTimeout=5 tt-zrh echo ok 2>/dev/null; then
    echo "  [OK] SSH to tt-zrh works"
else
    echo "  [WARN] SSH to tt-zrh failed (check ~ubuntu/.ssh/config)"
fi

# Check Redis
if redis-cli -h "$REDIS_HOST" -p "$REDIS_PORT" ping 2>/dev/null | grep -q PONG; then
    echo "  [OK] Redis is reachable"
else
    echo "  [WARN] Redis not reachable at ${REDIS_HOST}:${REDIS_PORT}"
fi

# Check S3 credentials are set
if [[ -n "$S3_ENDPOINT" && -n "$S3_ACCESS_KEY" && -n "$S3_SECRET_KEY" ]]; then
    echo "  [OK] S3 credentials configured"
else
    echo "  [WARN] S3 credentials not fully configured in .env"
    echo "         Edit /opt/pipeline/.env and set S3_ENDPOINT, S3_ACCESS_KEY, S3_SECRET_KEY"
fi

echo ""

# =============================================================================
# SUMMARY
# =============================================================================
echo ""
echo "=== Setup Complete ==="
echo ""
echo "Configuration:"
echo "  Python venv:  /opt/pipeline/venv"
echo "  Environment:  /opt/pipeline/.env"
echo "  Logs:         $LOG_DIR"
echo "  Service:      transfer-worker.service"
echo ""

if [[ $ERRORS -gt 0 ]]; then
    echo "ERRORS: $ERRORS issues detected (see above)"
    echo ""
fi

echo "Next steps:"
echo "  1. Verify .env has correct S3 credentials: cat /opt/pipeline/.env"
echo "  2. Verify SSH to tt-zrh: sudo -u ubuntu ssh tt-zrh 'ls /mnt/hub/export/sound_buffer/'"
echo "  3. Start the service:"
echo "       sudo systemctl enable --now transfer-worker"
echo ""
echo "Monitor:"
echo "  journalctl -u transfer-worker -f"
echo ""
echo "Health check:"
echo "  /opt/pipeline/check-health.sh"
echo ""
